1.hive连MySQL #已解决
#echo "skip-grant-tables" >> /etc/mysql/mysql.conf.d/mysqld.cnf && \不能加，加了不监听端口
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';
FLUSH PRIVILEGES;
RUN service mysql start && \
    mysql -u root -e "ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root'; FLUSH PRIVILEGES;"


#大概是hbase的问题

2.phbase开一半

3.habse启动问题
#可能是版本对不上


/opt/apache-hive-4.0.1-bin/bin/schematool -initSchema -dbType mysql
 vim /etc/mysql/mysql.conf.d/mysqld.cnf
cat /var/log/mysql/error.log
service mysql status






zookeeper部分
RUN sed -i 's/#dataDir=\/tmp\/zookeeper/dataDir=\/opt\/apache-zookeeper-3.8.4-bin/' /opt/apache-zookeeper-3.8.4-bin/conf/zoo.cfg && \
    echo 'server.0=localhost:2888:3888' >> /opt/apache-zookeeper-3.8.4-bin/conf/zoo.cfg
不知道这个端口是怎么配置的

#spark部分　上传jar
#RUN hdfs dfs -mkdir -p /user/spark/directory && \
    #hdfs dfs -put /opt/spark-3.5.6-bin-hadoop3/jars/* /user/spark/directory
不知道干什么 ai解释的挺高大上的

#RUN pip3 install phoenixdb
不想装，要pip3

hive部分
已解决		#RUN cp /opt/apache-hive-4.0.1-bin/hive-env.sh.template /opt/apache-hive-4.0.1-bin/hive-env.sh
			原本在虚拟机里面配这样写没问题，但是在dockerfile里面就写找不到，得进虚拟机里面看是不是得写进脚本里
			//路径错误

#没做，hive也启动成功了
#RUN hadoop fs -mkdir -p /user/hive/warehouse
#RUN hadoop fs -chmod -R 777 /user/hive/warehouse
#RUN hadoop fs -mkdir -p /tmp/hive/ 
#RUN hadoop fs -chmod -R 777 /tmp/hive
//先执行这些
flush privileges;
ALTER USER 'root'@'localhost' IDENTIFIED BY 'root';
flush privileges;
use mysql;
update user set Host='%' where User='root';
flush privileges;
才能跑通这个
#RUN /opt/apache-hive-4.0.1-bin/bin/schematool -initSchema -dbType mysql
可能要等到最后才能进行

#Hive整合Hbase
#RUN hbase shell
#create 'tablename', cf1'
#"habse.columns.mapping" = ":key,data:cf1"
建表映射
#./hive --service hiveserver2 -hiveconf hive.server2.thrift.port=10000
连pyhive

kafka部分
#RUN mkdir /opt/kafka_2.12-3.7.2/logs#感觉没必要

#已解决
kafka启动脚本//脚本规则如何
#!/bin/bash

case $1 in
"start")
        for host in localhost
        do
                echo "正在启动 $host Kafka 服务"
                ssh $host "/opt/kafka_2.12-3.7.2/bin/kafka-server-start.sh -daemon /opt/kafka_2.12-3.7.2/config/server.properties"
        done
        ;;
"stop")
        for host in localhost
        do
                echo "正在停止 $host Kafka 服务"
                ssh $host '/opt/kafka_2.12-3.7.2/bin/kafka-server-stop.sh'
        done
        ;;
*)
        echo "参数有误，请选择 【start】 或 【stop】！"
        ;;
esac
 chmod +x kf.sh


flink部分
#不知道有没有影响
#RUN sed -i 's/localhost:8081/0.0.0.0:8083/' /opt/flink-1.17.2/conf/masters
#或者lcoalhost:8083 如果都不行就把端口开回8081
#没改也没有影响访问webui

sed -i 's/#rest.bind-port: 8083-8090/rest.bind-port: 8080-8090/' /opt/flink-1.17.2/conf/flink-conf.yaml
#不知道为什么8080-8090不行，估计8080没被占就用了8080了

先配置conf，再连接8083，教程错了





#RUN service ssh start启动ssh
#RUN 
已启动#hdfs namenode -format && \
已启动#start-all.sh && \启动hadoop
已启动#zkServer.sh start && \启动zookeeper
已启动#/opt/spark-3.5.6-bin-hadoop3/sbin/start-all.sh 启动spark
已启动#/opt/hbase-2.5.11/bin/start-hbase.sh 启动hbase #从上往下启动到这发现已经启动了hbase/386 DataNode已存在
	//首个启动时不会出现问题，但依旧报错无法找到日志配置文件，目前来看无所谓
	//怎么hadoop启动的时候要连ssh这玩意不需要还不报错
	//他根本不需要datanode啊启动的时候就启动了hmaster和hregionserver这俩
	//报错了启动也正常不管他了
	//版本不匹配，老的是2.4的
	//启动后自杀
已启动#/opt/flink-1.17.2/bin/start-cluster.sh 启动flink
已启动#python2 /opt/phoenix-queryserver-6.0.0/bin/queryserver.py start
已启动#kf.sh 启动kafka 脚本自己写的
后台启动项目

    #hive,phbase,mysql不需要自启动
已启动#hive: /opt/apache-hive-4.0.1-bin/bin/hive
报错#/opt/phoenix-hbase-2.5-5.2.1-bin/bin/sqlline.py localhost
	//需要先从ssh启动到hbase，除了spark
	//启动后只能连接一半，最后的页面出不来不知道是不是phoneixdb的问题
	//
#MySQL:service mysql start 
    #bin/flume-ng agent -c ./conf -f ./conf/hdfs-avro.conf -n a1 -Dflume.root.logger=INFO,console
    //flume的启动，要ssh连，不启动


mysql远程登录
service start mysqld
flush privileges;
ALTER USER 'root'@'localhost' IDENTIFIED BY 'root';
flush privileges;
use mysql;
update user set Host='%' where User='root';
flush privileges;



dockercompose问题
容器名存在了gut下

